<!doctype html>
<html>

<head>
  <meta charset="utf-8">
  <title>Roko's Basilisk</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/main.css">
</head>

<body>
  <article class="main">
    <section class="lead">
      <h1>Roko's Basilisk</h1>
      <a data-ad data-ad-name="nick-bostrom-superintelligence" data-ad-placement="article-lead"></a>
      <p class="lead">
        Roko’s basilisk is a thought experiment proposed in 2010 by the user Roko on the Less Wrong community blog.
        Roko used ideas in decision theory to argue that a sufficiently powerful AI agent would have an incentive to
        torture anyone who imagined the agent but didn't work to bring the agent into existence. The argument was
        called a "basilisk" because merely hearing the argument would supposedly put you at risk of torture from this
        hypothetical agent — a basilisk in this context is any information that harms or endangers the people who hear it.
      </p>
    </section>
    <section>
      <a data-ad data-ad-name="ray-kurzweil-singularity" data-ad-placement="article-first-section"></a>
      <p>
        Roko's argument was broadly rejected on Less Wrong, with commenters objecting that an agent like the one Roko
        was describing would have no real reason to follow through on its threat: once the agent already exists, it can't
        affect the probability of its existence, so torturing people for their past decisions would be a waste of
        resources. Although several decision theories allow one to follow through on acausal threats and promises —
        via the same precommitment methods that permit mutual cooperation in prisoner's dilemmas — it is not clear
        that such theories can be blackmailed. If they can be blackmailed, this additionally requires a large amount
        of shared information and trust between the agents, which does not appear to exist in the case of Roko's basilisk.
      </p>
    </section>
    <section>
      <a data-ad data-ad-name="alex-davies-driven" data-ad-placement="article-second-section"></a>
      <p>
        Less Wrong's founder, Eliezer Yudkowsky, banned discussion of Roko's basilisk on the blog for several years as
        part of a general site policy against spreading potential information hazards. This had the opposite of its
        intended effect: a number of outside websites began sharing information about Roko's basilisk, as the ban
        attracted attention to this taboo topic. Websites like RationalWiki spread the assumption that Roko's basilisk
        had been banned because Less Wrong users accepted the argument; thus many criticisms of Less Wrong cite Roko's
        basilisk as evidence that the site's users have unconventional and wrong-headed beliefs.
      </p>
    </section>
  </article>

  <script src="js/ads-service.js"></script>
  <script src="js/ads-tracker.js"></script>
  <script src="js/main.js"></script>
</body>

</html>
